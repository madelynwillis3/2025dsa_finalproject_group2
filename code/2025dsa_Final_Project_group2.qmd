---
title: "2025dsa_Final_Project_group2"
format: html
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Madelyn Willis, Charles Appolon
---

## 1. Packages

```{r - load packages, warning = FALSE}
library(tidyverse)
library(USAboundaries) # for US state boundaries
library(sf) # for US map
library(daymetr)
library(dplyr)
library(purrr)
library(janitor)
library(ggridges)
library(GGally)
library(tidymodels)
library(xgboost)
library(finetune)  # for tune_race_anova
library(vip)
library(doParallel)
library(beepr)
library(doFuture)
library(ranger)
library(plotly)
library(shiny)
```

## 1a. Data Pull

```{r - train data pull}

clean_site_names <- function(site_col) { #to clean names
  site_col %>%
    # remove replicates like IAH1a
    gsub("([A-Z]+\\d+)\\w?$", "\\1", .) %>%
    # remove rep numbers and spaces
    gsub("-? ?rep \\d+$", "", .) %>%
    # Remove dry early and late
    gsub("- ?(Dry|Early|Late)$", "", .)
}

#meta:
train_meta <- read.csv("../data/training/training_meta.csv") %>%
  mutate(site = clean_site_names(site))
train_meta$year_site <- paste0(train_meta$site, "_", train_meta$year)
#does not have TXH4

#trait:
train_trait <- read.csv("../data/training/training_trait.csv") %>%
  mutate(site = clean_site_names(site))

#soil:
train_soil <- read.csv("../data/training/training_soil.csv")
train_soil <- train_soil %>%
  mutate(year_site = train_soil$site)
#does not have 2014

train_soil <- read.csv("../data/training/training_soil.csv") %>%
  mutate(
    year_site = site,
    year_site = gsub("_(\\d+)_", "_", year_site)
  ) %>%
  select(-c(year, site))

train_soil_avg <- train_soil %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop") %>%
  ungroup() 


train_trait$year_site <- paste0(train_trait$site, "_", train_trait$year)



train_meta_reduced <- train_meta %>%
  group_by(year_site) %>%
  summarise(
    latitude = mean(latitude), 
    longitude = mean(longitude),
    previous_crop = first(previous_crop),
    year = first(year),
    site = first(site),
    .groups = "drop"
  )

# # Pull weather data only once per site
# daymet_all <- train_meta_reduced %>%
#   mutate(weather = pmap(list(.y = year,
#                              .site = site,
#                              .lat = latitude,
#                              .lon = longitude),
#                         function(.y, .site, .lat, .lon)
#                           try({
#                             download_daymet(
#                               site = .site,
#                               lat = .lat,
#                               lon = .lon,
#                               start = .y,
#                               end = .y,
#                               simplify = TRUE,
#                               silent = TRUE
#                             ) })))




train_trait <- left_join(train_trait, train_meta_reduced, by = "year_site")

train_trait$previous_crop <- as.factor(train_trait$previous_crop)



df <- left_join(train_trait, train_soil_avg, by = "year_site")


na.omit(df[1:15])

df <- df %>%
  mutate(previous_crop = tolower(previous_crop)) %>%
  mutate(previous_crop = case_when(
           is.na(previous_crop) ~ NA_character_,
           str_detect(previous_crop, "soy") ~ "soybean",
           str_detect(previous_crop, "corn") ~ "corn",
           str_detect(previous_crop, "cotton") ~ "cotton",
           str_detect(previous_crop, "wheat") ~ "small grain",
           str_detect(previous_crop, "sorghum") ~ "sorghum",
           str_detect(previous_crop, "rye") ~ "small grain",
           str_detect(previous_crop, "peanut") ~ "peanut",
           str_detect(previous_crop, "beet") ~ "sugar beet",
           str_detect(previous_crop, "fallow") ~ "fallow",
           str_detect(previous_crop, "lima bean") ~ "small grain",
           str_detect(previous_crop, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  mutate(yield_mg_ha = yield_mg_ha * ((100 - grain_moisture) / (100 - 15.5))) %>%
  filter(!str_detect(year_site, "^GEH|ONH")) 





```

## 1b. Data Wrangling

```{r - wrangle weather data}
#gathering data: dropping data that doesn't work within daymet boundaries.
# daymet_all_w <- daymet_all %>%
#   filter(!map_lgl(weather, inherits, "try-error")) %>%
#   unnest(weather, names_sep = "_") %>%
#   group_by(
#     site,
#     year,
#     yday = weather_yday,
#     weather_measurement,
#     latitude = weather_latitude,
#     longitude = weather_longitude,
#     altitude = weather_altitude
#   ) %>%
#   summarise(
#     value = mean(weather_value, na.rm = TRUE),
#     .groups = "drop"
#   ) %>%
#   pivot_wider(
#     names_from = weather_measurement,
#     values_from = value
#   ) %>%
#   clean_names()
# 
#  daymet_all_w %>%
#   # Selecting needed variables
#   dplyr::select(year, site, latitude, longitude,
#                 yday,
#                 dayl.s = dayl_s,
#                 prcp.mm = prcp_mm_day,
#                 srad.wm2 = srad_w_m_2,
#                 tmax.c = tmax_deg_c,
#                 tmin.c = tmin_deg_c,
#                 vp.pa = vp_pa
#                 )
# 
# write.csv(daymet_all_w, "../data/training/training_weather.csv")


```

## 2. Feature Engineering

```{r - fe weather}
training_weather<- read.csv("../data/training/training_weather.csv")

training_weather %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")


training_weather <- training_weather %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))

training_weather_fe <- training_weather %>%
  group_by(year, site, month, month_abb) %>%  # include month_abb here
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()


training_weather_wide <- training_weather_fe %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))

# ggplot(data = training_weather_fe,
#        aes(x = mean_tmax_deg_c,
#            y = month_abb,
#            fill = stat(x))) +
#   geom_density_ridges_gradient(scale = 3,
#                                rel_min_height = 0.01) +
#   scale_fill_viridis_c(option = "D") + 
#   theme(legend.position = "none")

write_csv(training_weather_wide, "../data/training/training_weather_monthsum.csv")




```



```{r}

training_weather_wide <- read_csv("../data/training/training_weather_monthsum.csv")
# Add site_year
training_weather_wide <- training_weather_wide %>%
  mutate(year_site = paste0(site, "_", year))


# summarize weather to 1 row per year_site
training_weather_wide_summary <- training_weather_wide %>%
  select(-month) %>%  # exclude 'month' before summarizing
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join weather and df
df_full <- df %>%
  left_join(training_weather_wide_summary, by = "year_site") %>%
 # select(-c("year.x", "previous_crop", "year.y")) %>%
  mutate(year_site = gsub("TXH1-(Dry|Early|Late)", "TXH1", year_site))


```

```{r EDA}

library(stringr)
#lat lon coords per site
coord_lookup <- df_full %>%
  filter(!is.na(latitude) & !is.na(longitude)) %>%
  group_by(site.x) %>%
  summarise(lat_site = mean(latitude), lon_site = mean(longitude), .groups = "drop")

# fill NA sites with coordinates of filled sites
df_full <- df_full %>%
  left_join(coord_lookup, by = "site.x") %>%
  mutate(
    latitude = if_else(is.na(latitude), lat_site, latitude),
    longitude = if_else(is.na(longitude), lon_site, longitude)
  ) %>%
  select(-lat_site, -lon_site)

df_with_coords <- df_full %>% filter(!is.na(latitude) & !is.na(longitude))

# Get US states geometry
states <- us_states() %>%
  filter(!(state_abbr %in% c("PR", "AK", "HI")))

# Plot all points over U.S. map
# ggplot() +
#   geom_sf(data = states, fill = "white", color = "black") +
#   geom_point(data = df_with_coords,
#              aes(x = longitude, y = latitude),
#              color = "blue", size = 1.5) +
#   labs(title = "All Site Locations")

#clean up:
df_model <- df_full %>%
  select(-replicate, -block, -year.y, -site.y,-year.x, -site.x, -grain_moisture, -year )
write_csv(df_model, "../data/df_model.csv")

#there are many NAs in this dataset. Soils data is incomplete, non-US sites (like the Germany site) do not have weather data, and some sites are missing lat/lon coordinates. This is OK for XGBoost, so we will continue. After this we will run random forest after imputation.
```

## XGBoost
```{r}


set.seed(626)
options(future.globals.maxSize = 10 * 1024^3)  # 10GB

registerDoFuture()
plan(multisession, workers = 2)  # Adjust workers based on RAM


# get categorical columns
cat_cols <- df_training %>%
  select(where(is.factor)) %>%
  names()

# collapse rare levels in categorical vars (threshold: <5% of total)
df_training <- df_training %>%
  mutate(across(all_of(cat_cols), ~ fct_lump_prop(.x, prop = 0.05)))  # 5% threshold

# apply same levels to testing set to avoid new levels issue
df_testing <- df_testing %>%
  mutate(across(all_of(cat_cols), ~ fct_lump_prop(.x, prop = 0.05))) %>%
  mutate(across(all_of(cat_cols), ~ factor(.x, levels = levels(df_training[[cur_column()]]))))

#plot test vs train:
ggplot() +
  geom_density(data = df_training, aes(x = yield_mg_ha), color = "red") +
  geom_density(data = df_testing, aes(x = yield_mg_ha), color = "blue")


#recipe:
df_recipe <- recipe(yield_mg_ha ~ ., data = df_training) %>%
  step_nzv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)


#hyperparameter tune
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

#workflow:
xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(df_recipe)

#set folds to 10
resampling_foldcv <- vfold_cv(df_training, v = 5)

#grid
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 10  # 10, since 50 doesn't work on my laptop
)

registerDoFuture()
plan(multisession, workers = 2)  # reduce to 2-4 if hitting memory issues


xgb_res <- tune_race_anova(
  object = xgb_workflow,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_race(verbose_elim = TRUE, save_pred = TRUE)
)


```



```{r - best}
#rmse
best_rmse_xgb <- xgb_res %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_xgb")
best_rmse_xgb

#r2
best_r2_xgb <- xgb_res %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_xgb")
best_r2_xgb
#pct loss
best_rmse_pct_loss_xgb <- xgb_res %>%
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1) %>%
  mutate(source = "best_rmse_pct_loss_xgb")
best_rmse_pct_loss_xgb
#rmse based on 1 std error:
best_rmse_one_std_err_xgb <- xgb_res %>%
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_rmse_one_std_err_xgb")
best_rmse_one_std_err_xgb
```

```{r final run}
final_spec_xgb <- boost_tree(
  trees = best_r2_xgb$trees,
  tree_depth = best_r2_xgb$tree_depth,
  min_n = best_r2_xgb$min_n,
  learn_rate = best_r2_xgb$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

final_fit_xgb <- last_fit(final_spec_xgb,
                          df_recipe,
                          split = df_split)
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield_mg_ha_adj,
                              y = .pred)) + 
  geom_point() +
  geom_abline() + 
  geom_smooth(method = "lm") + 
  scale_x_continuous(limits = C(20,40)) +
  scale_y_continuous(limit = c(20,40)) 

```

## Random Forest
```{r RF}

# Optional: Enable parallel processing
registerDoParallel()

# split
set.seed(123)
df_split <- initial_split(df_model, prop = 0.7, strata = yield_mg_ha)
df_train <- training(df_split)
df_test  <- testing(df_split)

# recipe:
df_recipe <- recipe(yield_mg_ha ~ ., data = df_train) %>%                   # Replace with column names to drop
  step_naomit(all_predictors(), all_outcomes()) %>%  # Drop rows with NA
  step_zv(all_predictors())                          # Remove zero variance predictors

# 3. Define model spec
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = 5) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("regression")

# 4. Create CV folds
set.seed(34549)
cv_folds <- vfold_cv(df_train, v = 5)

# 5. Tune model with simulated annealing
set.seed(76544)
rf_tuned <- tune_sim_anneal(
  object = rf_spec,
  preprocessor = df_recipe,
  resamples = cv_folds,
  iter = 50
)
beepr::beep("wilhelm")
# 6. Plot tuning results
autoplot(rf_tuned)

# 7. Select best model based on RMSE
best_model <- select_best(rf_tuned, "rmse")

# 8. Finalize model and workflow
final_rf <- finalize_model(rf_spec, best_model)

wf <- workflow() %>%
  add_model(final_rf) %>%
  add_recipe(df_recipe)

# 9. Fit finalized model on training data
final_fit <- fit(wf, data = df_train)

# 10. Visualize variable importance
vip(final_fit$fit$fit)


```
```{r}

# models:

# model_final <- readRDS("xgboost_model.rds")

# df_model <- read_csv("your_processed_data.csv")

ui <- fluidPage(
  titlePanel("Modeling Corn Yield"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("variable", "Choose a variable to explore:", 
                  choices = names(df_model)[-which(names(df_model) == "yield")]),
      helpText("Select a variable to see its distribution.")
    ),
    
    mainPanel(
      tabsetPanel(
        tabPanel("Yield EDA", 
                 plotOutput("eda_yield_plot")),
        tabPanel("Predictor EDA", 
                 plotOutput("eda_variable_plot")),
        tabPanel("Variable Importance", 
                 plotOutput("vip_plot")),
        tabPanel("Predicted vs Observed", 
                 plotlyOutput("pred_obs_plot"),
                 verbatimTextOutput("metrics"))
      )
    )
  )
)

server <- function(input, output) {
  
  # EDA on yield
  output$eda_yield_plot <- renderPlot({
    ggplot(df_model, aes(x = yield)) +
      geom_histogram(fill = "steelblue", bins = 30) +
      labs(title = "Distribution of Yield", x = "Yield", y = "Count")
  })
  
  # EDA on chosen variable
  output$eda_variable_plot <- renderPlot({
    ggplot(df_model, aes_string(x = input$variable)) +
      geom_histogram(fill = "darkgreen", bins = 30) +
      labs(title = paste("Distribution of", input$variable), x = input$variable, y = "Count")
  })
  
  # Variable importance plot (replace with your model)
  output$vip_plot <- renderPlot({
    vip(model_final, num_features = 10) +
      labs(title = "Top 10 Important Variables")
  })
  
  # Predicted vs observed plot
  output$pred_obs_plot <- renderPlotly({
    # preds <- predict(model_final, new_data = df_model_test)$.pred
    # actuals <- df_model_test$yield
    # Replace above with real prediction and observed values
    
    ggplot(df_model_test, aes(x = actual_yield, y = predicted_yield)) +
      geom_point(alpha = 0.7) +
      geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
      labs(title = "Predicted vs Observed Yield",
           x = "Observed Yield", y = "Predicted Yield") +
      theme_minimal()
  })
  
  output$metrics <- renderPrint({
    # Replace with actual results
    rmse_val <- rmse(data = df_model_test, truth = actual_yield, estimate = predicted_yield)
    rsq_val <- rsq(data = df_model_test, truth = actual_yield, estimate = predicted_yield)
    list(RMSE = rmse_val, R2 = rsq_val)
  })
  
}

shinyApp(ui = ui, server = server)
```






