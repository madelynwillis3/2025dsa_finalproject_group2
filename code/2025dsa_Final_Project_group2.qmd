---
title: "2025dsa_Final_Project_group2"
format: html
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Madelyn Willis, Charles Appolon
---

## 1. Packages

```{r - load packages, warning = FALSe}
library(tidyverse)
library(USAboundaries) # for US state boundaries
library(sf) # for US map
library(daymetr)
library(dplyr)
library(purrr)
library(janitor)
library(ggridges)
library(GGally)
library(tidymodels)
library(xgboost)
library(finetune)  # for tune_race_anova
library(vip)
library(doParallel)
library(beepr)

```

## 1a. Data Pull

```{r - train data pull}

train_meta <- read.csv("../data/training/training_meta.csv")

train_soil <- read.csv("../data/training/training_soil.csv")
#soil does not have 2014 data

train_trait <- read.csv("../data/training/training_trait.csv")


train_meta$year_site <- paste0(train_meta$site, "_", train_meta$year)
train_trait$year_site <- paste0(train_trait$site, "_", train_trait$year)
train_trait <- left_join(train_trait, train_meta, by = "year_site")

# Get unique combinations to avoid pulling redundant data
unique_site_years <- train_meta %>%
  distinct(site, year, latitude, longitude)

# # Pull weather data only once per site
# daymet_all <- unique_site_years %>%
#   mutate(weather = pmap(list(.y = year,
#                              .site = site,
#                              .lat = latitude,
#                              .lon = longitude),
#                         function(.y, .site, .lat, .lon)
#                           try({
#                             download_daymet(
#                               site = .site,
#                               lat = .lat,
#                               lon = .lon,
#                               start = .y,
#                               end = .y,
#                               simplify = TRUE,
#                               silent = TRUE
#                             )
#                           })))




```

## 1b. Data Wrangling

```{r - wrangle weather data}
#gathering data: dropping data that doesn't work within daymet boundaries.
# daymet_all_w <- daymet_all %>%
#   filter(!map_lgl(weather, inherits, "try-error")) %>%
#   unnest(weather, names_sep = "_") %>%
#   group_by(
#     site,
#     year,
#     yday = weather_yday,
#     weather_measurement,
#     latitude = weather_latitude,
#     longitude = weather_longitude,
#     altitude = weather_altitude
#   ) %>%
#   summarise(
#     value = mean(weather_value, na.rm = TRUE),
#     .groups = "drop"
#   ) %>%
#   pivot_wider(
#     names_from = weather_measurement,
#     values_from = value
#   ) %>%
#   clean_names()
#  
#  daymet_all_w %>%
#   # Selecting needed variables
#   dplyr::select(year, site, latitude, longitude,
#                 yday,
#                 dayl.s = dayl_s, 
#                 prcp.mm = prcp_mm_day,
#                 srad.wm2 = srad_w_m_2, 
#                 tmax.c = tmax_deg_c, 
#                 tmin.c = tmin_deg_c,
#                 vp.pa = vp_pa
#                 ) 
# 
# write.csv(daymet_all_w, "../data/training/training_weather.csv")
# 

```

## 2. Feature Engineering

```{r - fe weather}
training_weather<- read.csv("../data/training/training_weather.csv")

training_weather %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")


training_weather <- training_weather %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))

training_weather_fe <- training_weather %>%
  group_by(year, site, month, month_abb) %>%  # include month_abb here
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()


training_weather_wide <- training_weather_fe %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))

ggplot(data = training_weather_fe,
       aes(x = mean_tmax_deg_c,
           y = month_abb,
           fill = stat(x))) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) +
  scale_fill_viridis_c(option = "D") + 
  theme(legend.position = "none")

write_csv(training_weather_wide, "../data/training/training_weather_monthsum.csv")




```

```{r - combine datasets}
#combining data frames:


train_soil <- train_soil %>%
  rename(year_site = site)

df <- left_join(train_trait, train_soil, by = "year_site") %>%
   select(-c("year.y", "site.y"))
na.omit(df[1:15])


uyear <- unique(df$year.x)

nsite <- unique(df$site.x)


df$previous_crop <- as.factor(df$previous_crop )
df <- df %>%
  mutate(previous_crop_clean = tolower(previous_crop)) %>%
  mutate(previous_crop_clean = case_when(
           is.na(previous_crop_clean) ~ NA_character_,
           str_detect(previous_crop_clean, "soy") ~ "soybean",
           str_detect(previous_crop_clean, "corn") ~ "corn",
           str_detect(previous_crop_clean, "cotton") ~ "cotton",
           str_detect(previous_crop_clean, "wheat") ~ "small grain",
           str_detect(previous_crop_clean, "sorghum") ~ "sorghum",
           str_detect(previous_crop_clean, "rye") ~ "small grain",
           str_detect(previous_crop_clean, "peanut") ~ "peanut",
           str_detect(previous_crop_clean, "beet") ~ "sugar beet",
           str_detect(previous_crop_clean, "fallow") ~ "fallow",
           str_detect(previous_crop_clean, "lima bean") ~ "lima bean",
           str_detect(previous_crop_clean, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop_clean, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  mutate(yield_mg_ha_adj = yield_mg_ha * ((100 - grain_moisture) / (100 - 15.5)))



```

```{r}
# Add site_year to both data frames and store them
training_weather_wide <- read_csv("../data/training/training_weather_monthsum.csv")

training_weather_wide <- training_weather_wide %>%
  mutate(site_year = paste0(site, "_", year))

df <- df %>%
  mutate(site_year = paste0(site.x, "_", year.x))

# summarize weather data to 1 row per site_year
training_weather_wide_summary <- training_weather_wide %>%
  select(-month) %>%  # exclude 'month' before summarizing
  group_by(site_year) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join summarized weather data with df
df_full <- df %>%
  left_join(training_weather_wide_summary, by = "site_year") %>%
  select(-c("year.x.x", "previous_crop", "year.y"))

```

```{r EDA}

library(stringr)
#lat lon coords per site
coord_lookup <- df_full %>%
  filter(!is.na(latitude) & !is.na(longitude)) %>%
  group_by(site.x) %>%
  summarise(lat_site = mean(latitude), lon_site = mean(longitude), .groups = "drop")

# fill NA sites with coordinates of filled sites
df_full <- df_full %>%
  left_join(coord_lookup, by = "site.x") %>%
  mutate(
    latitude = if_else(is.na(latitude), lat_site, latitude),
    longitude = if_else(is.na(longitude), lon_site, longitude)
  ) %>%
  select(-lat_site, -lon_site)

df_with_coords <- df_full %>% filter(!is.na(latitude) & !is.na(longitude))

# Get US states geometry
states <- us_states() %>%
  filter(!(state_abbr %in% c("PR", "AK", "HI")))

# Plot all points over U.S. map
# ggplot() +
#   geom_sf(data = states, fill = "white", color = "black") +
#   geom_point(data = df_with_coords,
#              aes(x = longitude, y = latitude),
#              color = "blue", size = 1.5) +
#   labs(title = "All Site Locations")


#there are many NAs in this dataset. Soils data is incomplete, non-US sites (like the Germany site) do not have weather data, and some sites are missing lat/lon coordinates. This is OK for XGBoost, so we will continue. After this we will run random forest after imputation.
```

## XGBoost
```{r}



set.seed(626)

#clean up:
df_model <- df_full %>%
  select(-replicate, -block, -year.x, -site.x, -site_year, -yield_mg_ha)

#split:
df_split <- initial_split(df_model, prop = 0.7, strata = yield_mg_ha_adj)
df_training <- training(df_split)
df_testing  <- testing(df_split)

#plot test vs train:
ggplot() + 
  geom_density(data = df_training, aes(x = yield_mg_ha_adj), color = "red") +
  geom_density(data = df_testing, aes(x = yield_mg_ha_adj), color = "blue")

#recipe:
df_recipe <- recipe(yield_mg_ha_adj ~ ., data = df_training) %>%
  step_nzv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%  # collapse rare hybrid levels
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

#specify xgb params:
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

#workflow:
xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(df_recipe)

#set folds to 10
resampling_foldcv <- vfold_cv(df_training, v = 10)

#grid
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 25  # 25, since 50 doesn't work on my laptop
)

registerDoFuture()
plan(multisession, workers = 4)  # reduce to 2-4 if hitting memory issues


xgb_res <- tune_race_anova(
  object = xgb_workflow,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_race(verbose_elim = TRUE, save_pred = TRUE)
)

write_csv(xgb_res, "../data/xgb_res.csv")
```

```{r - best}
#rmse
best_rmse_xgb <- xgb_res %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_xgb")
best_rmse_xgb

#r2
best_r2_xgb <- xgb_res %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_xgb")
best_r2_xgb
#pct loss
best_rmse_pct_loss_xgb <- xgb_res %>%
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1) %>%
  mutate(source = "best_rmse_pct_loss_xgb")
best_rmse_pct_loss_xgb
#rmse based on 1 std error:
best_rmse_one_std_err_xgb <- xgb_res %>%
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_rmse_one_std_err_xgb")
best_rmse_one_std_err_xgb
```

```{r final run}
final_spec_xgb <- boost_tree(
  trees = best_r2_xgb$trees,
  tree_depth = best_r2_xgb$tree_depth,
  min_n = best_r2_xgb$min_n,
  learn_rate = best_r2_xgb$learn_rate) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

final_fit_xgb <- last_fit(final_spec_xgb,
                          df_recipe,
                          split = df_split)
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield_mg_ha_adj,
                              y = .pred)) + 
  geom_point() +
  geom_abline() + 
  geom_smooth(method = "lm") + 
  scale_x_continuous(limits = C(20,40)) +
  scale_y_continuous(limit = c(20,40)) 

```








