---
title: "2025dsa_Final_Project_group2"
format: html
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Madelyn Willis, Charles Appolon
---

## 1. Packages

```{r - load packages, message = FALSE }
library(tidyverse)
library(USAboundaries) # for US state boundaries
library(sf) # for US map
library(daymetr)
library(dplyr)
library(purrr)
library(janitor)
library(ggridges)
library(GGally)
library(tidymodels)
library(xgboost)
library(finetune)  # for tune_race_anova
library(vip)
library(doParallel)
library(beepr)
library(doFuture)
library(ranger)
library(plotly)
library(shiny)
```

## 1a. Data Pull

```{r - train data pull}

clean_site_names <- function(site_col) { #to clean names
  site_col %>%
    # remove replicates like IAH1a
    gsub("([A-Z]+\\d+)\\w?$", "\\1", .) %>%
    # remove rep numbers and spaces
    gsub("-? ?rep \\d+$", "", .) %>%
    # Remove dry early and late
    gsub("- ?(Dry|Early|Late)$", "", .)
}

#meta:
train_meta <- read.csv("../data/training/training_meta.csv") %>%
  mutate(site = clean_site_names(site))
train_meta$year_site <- paste0(train_meta$site, "_", train_meta$year)
#does not have TXH4

#trait:
train_trait <- read.csv("../data/training/training_trait.csv") %>%
  mutate(site = clean_site_names(site))

#soil:
# train_soil <- read.csv("../data/training/training_soil.csv")
# train_soil <- train_soil %>%
#   mutate(year_site = train_soil$site)
# #does not have 2014

train_soil <- read.csv("../data/training/training_soil.csv") %>%
  mutate(
    year_site = site,
    year_site = gsub("_(\\d+)_", "_", year_site)
  ) %>%
  select(-c(year, site))

train_soil_avg <- train_soil %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop") %>%
  ungroup() 


train_trait$year_site <- paste0(train_trait$site, "_", train_trait$year)



train_meta_reduced <- train_meta %>%
  group_by(year_site) %>%
  summarise(
    latitude = mean(latitude), 
    longitude = mean(longitude),
    previous_crop = first(previous_crop),
    year = first(year),
    site = first(site),
    .groups = "drop"
  )

# Pull weather data only once per site
daymet_all <- train_meta_reduced %>%
  mutate(weather = pmap(list(.y = year,
                             .site = site,
                             .lat = latitude,
                             .lon = longitude),
                        function(.y, .site, .lat, .lon)
                          try({
                            download_daymet(
                              site = .site,
                              lat = .lat,
                              lon = .lon,
                              start = .y,
                              end = .y,
                              simplify = TRUE,
                              silent = TRUE
                            ) })))




train_trait <- left_join(train_trait, train_meta_reduced, by = "year_site")

train_trait$previous_crop <- as.factor(train_trait$previous_crop)



df <- left_join(train_trait, train_soil_avg, by = "year_site")


na.omit(df[1:15])

df <- df %>%
  mutate(previous_crop = tolower(previous_crop)) %>%
  mutate(previous_crop = case_when(
           is.na(previous_crop) ~ NA_character_,
           str_detect(previous_crop, "soy") ~ "soybean",
           str_detect(previous_crop, "corn") ~ "corn",
           str_detect(previous_crop, "cotton") ~ "cotton",
           str_detect(previous_crop, "wheat") ~ "small grain",
           str_detect(previous_crop, "sorghum") ~ "sorghum",
           str_detect(previous_crop, "rye") ~ "small grain",
           str_detect(previous_crop, "peanut") ~ "peanut",
           str_detect(previous_crop, "beet") ~ "sugar beet",
           str_detect(previous_crop, "fallow") ~ "fallow",
           str_detect(previous_crop, "lima bean") ~ "small grain",
           str_detect(previous_crop, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  mutate(yield_mg_ha = yield_mg_ha * ((100 - grain_moisture) / (100 - 15.5))) %>%
  filter(!str_detect(year_site, "^GEH|ONH")) 





```

## 1b. Data Wrangling

```{r - wrangle weather data}
#gathering data: dropping data that doesn't work within daymet boundaries.
daymet_all_w <- daymet_all %>%
  filter(!map_lgl(weather, inherits, "try-error")) %>%
  unnest(weather, names_sep = "_") %>%
  group_by(
    site,
    year,
    yday = weather_yday,
    weather_measurement,
    latitude = weather_latitude,
    longitude = weather_longitude,
    altitude = weather_altitude
  ) %>%
  summarise(
    value = mean(weather_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = weather_measurement,
    values_from = value
  ) %>%
  clean_names()

 daymet_all_w %>%
  # Selecting needed variables
  dplyr::select(year, site, latitude, longitude,
                yday,
                dayl.s = dayl_s,
                prcp.mm = prcp_mm_day,
                srad.wm2 = srad_w_m_2,
                tmax.c = tmax_deg_c,
                tmin.c = tmin_deg_c,
                vp.pa = vp_pa
                )

write.csv(daymet_all_w, "../data/training/training_weather.csv")


```

## 2. Feature Engineering

```{r - fe weather, warning = FALSE}
#pull:
training_weather<- read.csv("../data/training/training_weather.csv")
#pivot:
training_weather %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")

#fixing dates:
training_weather <- training_weather %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))
#group & summarize by variable
training_weather_fe <- training_weather %>%
  group_by(year, site, month, month_abb) %>%
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()

#filter by active months and clean up:
training_weather_wide <- training_weather_fe %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))

write_csv(training_weather_wide, "../data/training/training_weather_monthsum.csv")




```

```{r}
#pull data:
training_weather_wide <- read_csv("../data/training/training_weather_monthsum.csv")
# Add site_year:
training_weather_wide <- training_weather_wide %>%
  mutate(year_site = paste0(site, "_", year))


# summarize weather to 1 row per year_site:
training_weather_wide_summary <- training_weather_wide %>%
  select(-month) %>%  # exclude 'month' before summarizing
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join weather and df
df_full <- df %>%
  left_join(training_weather_wide_summary, by = "year_site") %>%
 # select(-c("year.x", "previous_crop", "year.y")) %>%
  mutate(year_site = gsub("TXH1-(Dry|Early|Late)", "TXH1", year_site))


```

```{r EDA}

library(stringr)
#lat lon coords per site
coord_lookup <- df_full %>%
  filter(!is.na(latitude) & !is.na(longitude)) %>%
  group_by(site.x) %>%
  summarise(lat_site = mean(latitude), lon_site = mean(longitude), .groups = "drop")

# fill NA sites with coordinates of filled sites
df_full <- df_full %>%
  left_join(coord_lookup, by = "site.x") %>%
  mutate(
    latitude = if_else(is.na(latitude), lat_site, latitude),
    longitude = if_else(is.na(longitude), lon_site, longitude)
  ) %>%
  select(-lat_site, -lon_site)

df_with_coords <- df_full %>% filter(!is.na(latitude) & !is.na(longitude))

# Get US states geometry
states <- us_states() %>%
  filter(!(state_abbr %in% c("PR", "AK", "HI")))

# Plot all points over U.S. map
# ggplot() +
#   geom_sf(data = states, fill = "white", color = "black") +
#   geom_point(data = df_with_coords,
#              aes(x = longitude, y = latitude),
#              color = "blue", size = 1.5) +
#   labs(title = "All Site Locations")

# Clean up and process the data
df_model <- df_full %>%
  # Extract the month from date_planted and date_harvested. This is to maintain month in the final model without it being a date format.
  mutate(
    month_planted = sub("^([0-9]+)/.*", "\\1", date_planted), # Extract the month from date_planted
    month_harvested = sub("^([0-9]+)/.*", "\\1", date_harvested), # Extract the month from date_harvested
    days_difference = as.numeric(as.Date(date_harvested, format = "%m/%d/%y") - as.Date(date_planted, format = "%m/%d/%y"))
  ) %>%
  # Remove unwanted columns
  select(
    -replicate, -block, -year.y, -site.y, -year.x, -site.x,
    -grain_moisture, -year, -date_planted, -date_harvested, -year_site
  ) %>%
 #remove all NAs in data:
  filter(!is.na(month_harvested))

# Write the resulting data frame to a CSV file
write_csv(df_model, "../data/df_model.csv")
#there are many NAs in this dataset. Soils data is incomplete, non-US sites (like the Germany site) do not have weather data, and some sites are missing lat/lon coordinates. This is OK for XGBoost, so we will continue. After this we will run random forest after imputation.
```

```{r, eval = FALSE}

#STILL NOT WORKING:
set.seed(626)
options(future.globals.maxSize = 10 * 1024^3)  # 10GB


# # split
set.seed(123)
df_split <- initial_split(df_model, prop = 0.7, strata = yield_mg_ha)
df_train <- training(df_split)
df_test  <- testing(df_split)

# get categorical columns

# cat_cols <- df_train %>%

cat_cols <- df_model %>%
  select(where(is.factor)) %>%
  names()

# collapse rare levels in categorical vars (threshold: <5% of total)

df_train <- df_train %>%

df_model <- df_model %>%
  mutate(across(all_of(cat_cols), ~ fct_lump_prop(.x, prop = 0.05)))  # 5% threshold

# apply same levels to testing set to avoid new levels issue
df_test <- df_test %>%
  mutate(across(all_of(cat_cols), ~ fct_lump_prop(.x, prop = 0.05))) %>%
  mutate(across(all_of(cat_cols), ~ factor(.x, levels = levels(df_train[[cur_column()]]))))

#plot test vs train:
ggplot() +
  geom_density(data = df_train, aes(x = yield_mg_ha), color = "red") +
  geom_density(data = df_test, aes(x = yield_mg_ha), color = "blue")


#recipe:
df_recipe <- recipe(yield_mg_ha ~ ., data = df_train) %>%
  step_novel() %>%
  step_unknown(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.001) %>%
  step_dummy(all_nominal_predictors()) %>%
  mutate(across(all_of(cat_cols), ~ factor(.x, levels = levels(df_model[[cur_column()]]))))

#plot test vs train:
ggplot() +
  geom_density(data = df_model, aes(x = yield_mg_ha), color = "red") +
  geom_density(data = df_model, aes(x = yield_mg_ha), color = "blue")


#recipe:
df_recipe <- recipe(yield_mg_ha ~ ., data = df_model) %>%

  step_nzv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors())

df_bake <- df_recipe %>%
  prep() %>%
  bake(new_data = NULL)%>%
  select(-ends_with("_other")) #other columns are not useful.


xgb_spec <- boost_tree(
  trees = 100,         # Fixed number of trees
  tree_depth = 5,      # Fixed tree depth
  min_n = 10,          # Fixed minimum number of observations
  learn_rate = 0.1     # Fixed learning rate
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(df_recipe)

xgb_fit <- fit(xgb_workflow, data = df_train)
#set folds to 5
resampling_foldcv <- vfold_cv(df_train, v = 5)

# Create a tuning grid containing only `trees`
xgb_grid <- grid_latin_hypercube(
  trees(range = c(50, 200)),
  size = 4 
)





# Set up parallel processing
cl <- makeCluster(6)  # Adjust the number of cores based on your system
registerDoParallel(cl)

start_x <- Sys.time()
# Tune the model
xgb_res <- tune_grid(
  object = xgb_workflow,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_race(verbose_elim = TRUE, save_pred = TRUE)
)

stopCluster(cl)  # Stop the cluster after tuning
end_x <- Sys.time()

start_x - end_x
```

## 3. Preprocessing

```{r}
# Load required libraries
library(tidymodels)
library(doParallel)

# Set seed for reproducibility
set.seed(123)

# Split the data
df_split <- initial_split(df_model, prop = 0.7, strata = yield_mg_ha)
df_train <- training(df_split) %>%
  select(-c(month_planted, month_harvested, days_difference))
df_test <- testing(df_split)

# Define the recipe for preprocessing
df_recipe <- recipe(yield_mg_ha ~ ., data = df_train) %>%
  #step_rm(c(month_planted, month_harvested, days_difference)) %>% # Remove unwanted columns
  step_novel(all_nominal_predictors()) %>%         # Handle new levels explicitly
  step_unknown(all_nominal_predictors()) %>%      # Handle unknown levels
  step_other(all_nominal_predictors(), threshold = 0.001) %>% # Group infrequent levels
  step_dummy(all_nominal_predictors()) %>%        # One-hot encode nominal predictors
  step_nzv(all_predictors()) %>%                  # Remove near-zero variance predictors
  step_impute_median(all_numeric_predictors())     # Impute missing numeric values
prep(df_recipe) %>% # Prepare the recipe
bake(new_data = NULL) %>% # Apply the recipe to the training data
  select(-ends_with("_other")) # Remove "_other" columns
```

## 4. XGBoost

```{r}
# Define the XGBoost model with tunable hyperparameters
xgb_spec <- boost_tree(
  trees = tune(),          # Number of trees to tune
  tree_depth = 5,          # Fixed tree depth
  min_n = 10,              # Fixed minimum number of observations
  learn_rate = 0.1         # Fixed learning rate
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Create the workflow
xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(df_recipe)

# Create 5-fold cross-validation
resampling_foldcv <- vfold_cv(df_train, v = 5)

# Create a tuning grid for the number of trees
xgb_grid <- grid_latin_hypercube(
  trees(range = c(50, 200)), # Range of trees to tune
  size = 4                   # Number of grid points
)

# Set up parallel processing (optional but recommended for speeding up the tuning process)
# num_cores <- parallel::detectCores() - 1
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)

# Tune the model
xgb_res <- tune_grid(
  object = xgb_workflow,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_race(verbose = TRUE, save_pred = TRUE)
)
#save rds:
saveRDS(xgb_res, "../data/xgb_res.rds")
# Stop the parallel cluster after tuning
# stopCluster(cl)
xgb_res <- readRDS("../data/xgb_res.rds")

#rmse
best_rmse_xgb <- xgb_res %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_xgb")
best_rmse_xgb

#r2
best_r2_xgb <- xgb_res %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_xgb")
best_r2_xgb
#pct loss
best_rmse_pct_loss_xgb <- xgb_res %>%
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1) %>%
  mutate(source = "best_rmse_pct_loss_xgb")
best_rmse_pct_loss_xgb
#rmse based on 1 std error:
best_rmse_one_std_err_xgb <- xgb_res %>%
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_rmse_one_std_err_xgb")
best_rmse_one_std_err_xgb
#based on this, the rmse one will be chosen, since 3/4 bests are the same model.

# Extract the best hyperparameters from the tuning results
best_params <- best_rmse_xgb

# Finalize the XGBoost specification with the best hyperparameters
xgb_final_spec <- boost_tree(
  trees = best_params$trees,
  tree_depth = 5,
  min_n = 10,
  learn_rate = 0.1
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Finalize the workflow with the best model
xgb_final_workflow <- workflow() %>%
  add_model(xgb_final_spec) %>%
  add_recipe(df_recipe)

# Fit the finalized workflow on the full training data
xgb_final_fit <- fit(xgb_final_workflow, data = df_train)

# Evaluate performance on the test set
test_predictions <- predict(xgb_final_fit, new_data = df_test) %>%
  bind_cols(df_test) %>%
  mutate(
    rmse = sqrt(mean((yield_mg_ha - .pred)^2, na.rm = TRUE)),
    rsq = cor(yield_mg_ha, .pred, use = "complete.obs")^2
  )

# Print RMSE and R-squared
test_rmse <- test_predictions$rmse[1]
test_rsq <- test_predictions$rsq[1]
cat("Test RMSE:", test_rmse, "\n")
cat("Test R-squared:", test_rsq, "\n")

# Calculate RMSE and R^2
rmse <- sqrt(mean((test_predictions$yield_mg_ha - test_predictions$.pred)^2, na.rm = TRUE))
rsq <- cor(test_predictions$yield_mg_ha, test_predictions$.pred, use = "complete.obs")^2

# Plot actual vs. predicted yield
library(ggplot2)
results_xgb<- ggplot(test_predictions, aes(x = yield_mg_ha, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Actual vs Predicted Yield (mg/ha)",
    x = "Actual Yield",
    y = "XgBoost Predicted Yield"
  ) +
  # Add RMSE and R² as text annotations (move up and to the left)
  annotate(
    "text",
    x = max(test_predictions$yield_mg_ha, na.rm = TRUE) * 0.2, # Move to the left (20% of max x-value)
    y = max(test_predictions$.pred, na.rm = TRUE) * 1.2,       # Move upward (80% of max y-value)
    label = paste0("RMSE: ", round(rmse, 2), "\nR²: ", round(rsq, 2)),
    color = "black",
    size = 4,
    hjust = 0
  ) +
  theme_minimal()
ggsave("../output/xgb_results.png", plot = results_xgb, width = 8, height = 6)
```

## 5. Random Forest

```{r}
num_predictors <- ncol(df_train) - 1  # Exclude the target variable
typical_mtry <- round(sqrt(num_predictors))

rf_spec <- rand_forest(
  mtry = 8,  # Fixed typical value
  trees = tune(),       # Only tune trees
  min_n = 5
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("regression")

# Create 5-fold cross-validation
cv_folds <- vfold_cv(df_train, v = 5)

# Create a tuning grid for trees
rf_grid <- grid_regular(
  trees(range = c(50, 500)),  # Tuning trees from 50 to 500
  levels = 5                 
)
tictoc::tic("rf_tuning")
# Tune the model using the updated recipe and cross-validation
rf_tuned <- tune_grid(
  object = rf_spec,
  preprocessor = df_recipe,
  resamples = cv_folds,
  grid = rf_grid,
  control = control_race(verbose = TRUE, save_pred = TRUE)
)
tictoc::toc()
# Notify when tuning is complete
beepr::beep("wilhelm")
# Save the tuned model results
saveRDS(rf_tuned, "../output/rf_tuned.rds")
rf_tuned <- readRDS("../output/rf_tuned.rds")
# Visualize tuning results
autoplot(rf_tuned)

best_rmse_rf <- rf_tuned %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_rf")
best_rmse_rf
#r2
best_r2_rf <- rf_tuned %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_rf")
best_r2_rf
#best r2 1 std
best_r2_one_std_rf <- rf_tuned %>%
  select_by_one_std_err(metric = "rsq",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_r2_one_std_rf")
best_r2_one_std_rf
# Select the best model based on RMSE
#best_model <- select_best(rf_tuned, "rmse")
best_model <- best_rmse_rf
# Finalize the random forest model with the best parameters
final_rf <- finalize_model(rf_spec, best_model)

# Create a workflow with the finalized model and recipe
wf <- workflow() %>%
  add_model(final_rf) %>%
  add_recipe(df_recipe)

# Fit the finalized workflow on the full training data
final_fit <- fit(wf, data = df_train)

# Evaluate variable importance
vip(final_fit$fit$fit)




# Evaluate performance on the test set
test_predictions_rf <- predict(final_fit, new_data = df_test) %>%
  bind_cols(df_test) %>%
  mutate(
    rmse = sqrt(mean((yield_mg_ha - .pred)^2, na.rm = TRUE)),
    rsq = cor(yield_mg_ha, .pred, use = "complete.obs")^2
  )

# Print RMSE and R-squared
test_rmse <- test_predictions_rf$rmse[1]
test_rsq <- test_predictions_rf$rsq[1]
cat("Test RMSE:", test_rmse, "\n")
cat("Test R-squared:", test_rsq, "\n")

# # Calculate RMSE and R^2
# rmse <- sqrt(mean((test_predictions_rf$yield_mg_ha - test_predictions_rf$.pred)^2, na.rm = TRUE))
# rsq <- cor(test_predictions_rf$yield_mg_ha, test_predictions_rf$.pred, use = "complete.obs")^2

# Plot actual vs. predicted yield
library(ggplot2)
results_rf <- ggplot(test_predictions_rf, aes(x = yield_mg_ha, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Actual vs Predicted Yield (mg/ha)",
    x = "Actual Yield",
    y = "Random Forest Predicted Yield"
  ) +
  # Add RMSE and R² as text annotations (move up and to the left)
  annotate(
    "text",
    x = max(test_predictions_rf$yield_mg_ha, na.rm = TRUE) * 0.2, # Move to the left (20% of max x-value)
    y = max(test_predictions_rf$.pred, na.rm = TRUE) * 1.2,       # Move upward (80% of max y-value)
    label = paste0("RMSE: ", round(rmse, 2), "\nR²: ", round(rsq, 2)),
    color = "black",
    size = 4,
    hjust = 0
  ) +
  theme_minimal()
ggsave("../output/rf_results.png", plot = results_rf, width = 8, height = 6)
```

## 6. Variable Importance

```{r Variable Importance}
prep<- prep(df_recipe)

#rf variabe importance
final_rf_plot <- final_rf %>%
  fit(yield_mg_ha ~ ., data = bake(prep, new_data = df_train)) %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
#save as object:

ggsave("../output/variable_importance_rf.png", plot = final_rf_plot, width = 8, height = 6, dpi = 300)


# XGBoost variable importance
xgb_final_spec %>%
  fit(yield_mg_ha ~ ., data = bake(prep, new_data = df_train)) %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

ggsave("../output/variable_importance_xgb.png", width = 8, height = 6, dpi = 300)
```

## 7. Predicting on Test Data

```{r - xgb on test data}



#NOT WORKING AT BOTTOM
clean_site_names <- function(site_col) { #to clean names
  site_col %>%
    # remove replicates like IAH1a
    gsub("([A-Z]+\\d+)\\w?$", "\\1", .) %>%
    # remove rep numbers and spaces
    gsub("-? ?rep \\d+$", "", .) %>%
    # Remove dry early and late
    gsub("- ?(Dry|Early|Late)$", "", .)
}

#meta:
test_meta <- read.csv("../data/testing/testing_meta.csv") %>%
  mutate(site = clean_site_names(site))
test_meta$year_site <- paste0(test_meta$site, "_", test_meta$year)
#does not have TXH4

#trait:
test_trait <- read.csv("../data/testing/testing_submission.csv") %>%
  mutate(site = clean_site_names(site))

#soil:

test_soil <- read.csv("../data/testing/testing_soil.csv") %>%
  mutate(
    year_site = site,
    year_site = gsub("_(\\d+)_", "_", year_site)
  ) %>%
  select(-c(year, site))

test_soil_avg <- test_soil %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop") %>%
  ungroup() 


test_trait$year_site <- paste0(test_trait$site, "_", test_trait$year)



test_meta_reduced <- test_meta %>%
  group_by(year_site) %>%
  summarise(
    latitude = mean(latitude), 
    longitude = mean(longitude),
    previous_crop = first(previous_crop),
    year = first(year),
    site = first(site),
    .groups = "drop"
  )

# # Pull weather data only once per site
daymet_test <- test_meta_reduced %>%
  mutate(weather = pmap(list(.y = year,
                             .site = site,
                             .lat = latitude,
                             .lon = longitude),
                        function(.y, .site, .lat, .lon)
                          try({
                            download_daymet(
                              site = .site,
                              lat = .lat,
                              lon = .lon,
                              start = .y,
                              end = .y,
                              simplify = TRUE,
                              silent = TRUE
                            ) })))




test_trait <- left_join(test_trait, test_meta_reduced, by = "year_site")

test_trait$previous_crop <- as.factor(test_trait$previous_crop)



df_test <- left_join(test_trait, test_soil_avg, by = "year_site")


na.omit(df[1:15])

df_test <- df_test %>%
  mutate(previous_crop = tolower(previous_crop)) %>%
  mutate(previous_crop = case_when(
           is.na(previous_crop) ~ NA_character_,
           str_detect(previous_crop, "soy") ~ "soybean",
           str_detect(previous_crop, "corn") ~ "corn",
           str_detect(previous_crop, "cotton") ~ "cotton",
           str_detect(previous_crop, "wheat") ~ "small grain",
           str_detect(previous_crop, "sorghum") ~ "sorghum",
           str_detect(previous_crop, "rye") ~ "small grain",
           str_detect(previous_crop, "peanut") ~ "peanut",
           str_detect(previous_crop, "beet") ~ "sugar beet",
           str_detect(previous_crop, "fallow") ~ "fallow",
           str_detect(previous_crop, "lima bean") ~ "small grain",
           str_detect(previous_crop, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  filter(!str_detect(year_site, "^GEH|ONH")) 

#gathering data: dropping data that doesn't work within daymet boundaries.
daymet_all_test <- daymet_test %>%
  filter(!map_lgl(weather, inherits, "try-error")) %>%
  unnest(weather, names_sep = "_") %>%
  group_by(
    site,
    year,
    yday = weather_yday,
    weather_measurement,
    latitude = weather_latitude,
    longitude = weather_longitude,
    altitude = weather_altitude
  ) %>%
  summarise(
    value = mean(weather_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = weather_measurement,
    values_from = value
  ) %>%
  clean_names()

 daymet_all_test %>%
  # Selecting needed variables
  dplyr::select(year, site, latitude, longitude,
                yday,
                dayl.s = dayl_s,
                prcp.mm = prcp_mm_day,
                srad.wm2 = srad_w_m_2,
                tmax.c = tmax_deg_c,
                tmin.c = tmin_deg_c,
                vp.pa = vp_pa
                )

daymet_all_test %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")


daymet_all_test <- daymet_all_test %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))

training_weather_fe_test <- daymet_all_test %>%
  group_by(year, site, month, month_abb) %>%  # include month_abb here
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()


training_weather_wide_test <- training_weather_fe_test %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))


# Add site_year
training_weather_wide_test <- training_weather_wide_test %>%
  mutate(year_site = paste0(site, "_", year))


# summarize weather to 1 row per year_site
training_weather_wide_summary_test <- training_weather_wide_test %>%
  select(-month) %>%  # exclude 'month' before summarizing
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join weather and df
df_full_test <- df_test %>%
  left_join(training_weather_wide_summary_test, by = "year_site") %>%
 # select(-c("year.x", "previous_crop", "year.y")) %>%
  mutate(year_site = gsub("TXH1-(Dry|Early|Late)", "TXH1", year_site))

baked_df_test <- prep(df_recipe) %>% # Prepare the recipe
bake(new_data = df_full_test) %>% # Apply the recipe to the training data
  select(-ends_with("_other")) # Remove "_other" columns



# populate predictions
test_sub <- read_csv("../data/testing/testing_submission.csv")
predictions <- predict(xgb_final_workflow, df_full_test) %>%
  bind_cols(df_test) 
predictions <- predictions %>%
  mutate(yield_mg_ha = .pred) %>%
  mutate(year = year.x) %>%
  mutate(site = site.x)
test_sub_updated <- test_sub %>%
  left_join(predictions %>% select(year, site, hybrid, yield_mg_ha), 
            by = c("year", "site", "hybrid")) %>%# Populate `yield` column with `yield_mg_ha`
  select(-yield_mg_ha.x) %>%
  mutate(yield_mg_ha = yield_mg_ha.y) %>%
  select(-yield_mg_ha.y)

# Write the updated submission file:
write_csv(test_sub_updated, "../output/testing_submission_xgb_WILLIS.csv")

#RF:
# populate predictions

predictions_rf <- predict(final_fit, df_full_test) %>%
  bind_cols(df_test) 
predictions_rf <- predictions_rf %>%
  mutate(yield_mg_ha = .pred) %>%
  mutate(year = year.x) %>%
  mutate(site = site.x)
test_sub_updated_rf <- test_sub %>%
  left_join(predictions_rf %>% select(year, site, hybrid, yield_mg_ha), 
            by = c("year", "site", "hybrid")) %>%# Populate `yield` column with `yield_mg_ha`
  select(-yield_mg_ha.x) %>%
  mutate(yield_mg_ha = yield_mg_ha.y) %>%
  select(-yield_mg_ha.y)

write_csv(test_sub_updated_rf, "../output/testing_submission_rf_WILLIS.csv")

```
