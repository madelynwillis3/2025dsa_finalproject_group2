---
title: "2025dsa_Final_Project_group2"
format: 
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Madelyn Willis
---

## 1. Packages

```{r - load packages, message = FALSE }
library(tidyverse)
library(USAboundaries)
library(sf)
library(daymetr)
library(dplyr)
library(purrr)
library(janitor)
library(ggridges)
library(GGally)
library(tidymodels)
library(xgboost)
library(finetune)
library(vip)
library(doParallel)
library(beepr)
library(doFuture)
library(ranger)
library(plotly)
library(shiny)
```

## 1a. Data Pull

```{r - train data pull, warning = FALSE, message=FALSE}

clean_site_names <- function(site_col) { #to clean names
  site_col %>%
    # remove replicates like IAH1a
    gsub("([A-Z]+\\d+)\\w?$", "\\1", .) %>%
    # remove rep numbers and spaces
    gsub("-? ?rep \\d+$", "", .) %>%
    # Remove dry early and late
    gsub("- ?(Dry|Early|Late)$", "", .)
}

#meta:
train_meta <- read.csv("../data/training/training_meta.csv") %>%
  mutate(site = clean_site_names(site))
train_meta$year_site <- paste0(train_meta$site, "_", train_meta$year)
  #does not have TXH4

#trait:
train_trait <- read.csv("../data/training/training_trait.csv") %>%
  mutate(site = clean_site_names(site))

#pull soil:

  #does not have 2014
train_soil <- read.csv("../data/training/training_soil.csv") %>%
  mutate(
    year_site = site,
    year_site = gsub("_(\\d+)_", "_", year_site)
  ) %>%
  select(-c(year, site))
#group by year site
train_soil_avg <- train_soil %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop") %>%
  ungroup() 

#add year site
train_trait$year_site <- paste0(train_trait$site, "_", train_trait$year)


#summarize
train_meta_reduced <- train_meta %>%
  group_by(year_site) %>%
  summarise(
    latitude = mean(latitude), 
    longitude = mean(longitude),
    previous_crop = first(previous_crop),
    year = first(year),
    site = first(site),
    .groups = "drop"
  )

# Pull weather data only once per site:
daymet_all <- train_meta_reduced %>%
  mutate(weather = pmap(list(.y = year,
                             .site = site,
                             .lat = latitude,
                             .lon = longitude),
                        function(.y, .site, .lat, .lon)
                          try({
                            download_daymet(
                              site = .site,
                              lat = .lat,
                              lon = .lon,
                              start = .y,
                              end = .y,
                              simplify = TRUE,
                              silent = TRUE
                            ) })))




train_trait <- left_join(train_trait, train_meta_reduced, by = "year_site")

train_trait$previous_crop <- as.factor(train_trait$previous_crop)



df <- left_join(train_trait, train_soil_avg, by = "year_site")


na.omit(df[1:15])

df <- df %>%
  mutate(previous_crop = tolower(previous_crop)) %>%
  mutate(previous_crop = case_when(
           is.na(previous_crop) ~ NA_character_,
           str_detect(previous_crop, "soy") ~ "soybean",
           str_detect(previous_crop, "corn") ~ "corn",
           str_detect(previous_crop, "cotton") ~ "cotton",
           str_detect(previous_crop, "wheat") ~ "small grain",
           str_detect(previous_crop, "sorghum") ~ "sorghum",
           str_detect(previous_crop, "rye") ~ "small grain",
           str_detect(previous_crop, "peanut") ~ "peanut",
           str_detect(previous_crop, "beet") ~ "sugar beet",
           str_detect(previous_crop, "fallow") ~ "fallow",
           str_detect(previous_crop, "lima bean") ~ "small grain",
           str_detect(previous_crop, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  mutate(yield_mg_ha = yield_mg_ha * ((100 - grain_moisture) / (100 - 15.5))) %>%
  filter(!str_detect(year_site, "^GEH|ONH")) 





```

## 1b. Data Wrangling

```{r - wrangle weather data, message=FALSE}

#gathering data: dropping data that doesn't work within daymet boundaries.
daymet_all_w <- daymet_all %>%
  filter(!map_lgl(weather, inherits, "try-error")) %>%
  unnest(weather, names_sep = "_") %>%
  group_by(
    site,
    year,
    yday = weather_yday,
    weather_measurement,
    latitude = weather_latitude,
    longitude = weather_longitude,
    altitude = weather_altitude
  ) %>%
  summarise(
    value = mean(weather_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = weather_measurement,
    values_from = value
  ) %>%
  clean_names()

 daymet_all_w %>%
  # Selecting needed variables
  dplyr::select(year, site, latitude, longitude,
                yday,
                dayl.s = dayl_s,
                prcp.mm = prcp_mm_day,
                srad.wm2 = srad_w_m_2,
                tmax.c = tmax_deg_c,
                tmin.c = tmin_deg_c,
                vp.pa = vp_pa
                )

write.csv(daymet_all_w, "../data/training/training_weather.csv")


```

## 2. Feature Engineering

```{r - fe weather, warning = FALSE}
#pull:
training_weather<- read.csv("../data/training/training_weather.csv")
#pivot:
training_weather %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")

#fixing dates:
training_weather <- training_weather %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))
#group & summarize by variable
training_weather_fe <- training_weather %>%
  group_by(year, site, month, month_abb) %>%
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()

#filter by active months and clean up:
training_weather_wide <- training_weather_fe %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))

write_csv(training_weather_wide, "../data/training/training_weather_monthsum.csv")




```

```{r - Join weather and df, message= FALSE, warning = FALSE}
#pull data:
training_weather_wide <- read_csv("../data/training/training_weather_monthsum.csv")
# Add site_year:
training_weather_wide <- training_weather_wide %>%
  mutate(year_site = paste0(site, "_", year))


# summarize weather to 1 row per year_site:
training_weather_wide_summary <- training_weather_wide %>%
  select(-month) %>%  # exclude 'month' before summarizing
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join weather and df
df_full <- df %>%
  left_join(training_weather_wide_summary, by = "year_site") %>%
 # select(-c("year.x", "previous_crop", "year.y")) %>%
  mutate(year_site = gsub("TXH1-(Dry|Early|Late)", "TXH1", year_site))


```

```{r - Fixing joined data}

library(stringr)
#lat lon coords per site
coord_lookup <- df_full %>%
  filter(!is.na(latitude) & !is.na(longitude)) %>%
  group_by(site.x) %>%
  summarise(lat_site = mean(latitude), lon_site = mean(longitude), .groups = "drop")

# some sites do not have coords. Filling them with the coords of other rows that do include them (with same site and year)
df_full <- df_full %>%
  left_join(coord_lookup, by = "site.x") %>%
  mutate(
    latitude = if_else(is.na(latitude), lat_site, latitude),
    longitude = if_else(is.na(longitude), lon_site, longitude)
  ) %>%
  select(-lat_site, -lon_site)

df_with_coords <- df_full %>% filter(!is.na(latitude) & !is.na(longitude))

# Get US geom
states <- us_states() %>%
  filter(!(state_abbr %in% c("PR", "AK", "HI")))

#optional to plot graph:
# Plot all points over U.S. map
# ggplot() +
#   geom_sf(data = states, fill = "white", color = "black") +
#   geom_point(data = df_with_coords,
#              aes(x = longitude, y = latitude),
#              color = "blue", size = 1.5) +
#   labs(title = "All Site Locations")

# Clean up and process the data: (didn't work out in the end but its good to include to see my process)

df_model <- df_full %>%
  # Extract the month from date_planted and date_harvested. This is to keep month in the final model without it being a date format.
  mutate(
    month_planted = sub("^([0-9]+)/.*", "\\1", date_planted), # Extract the month from date_planted
    month_harvested = sub("^([0-9]+)/.*", "\\1", date_harvested), # Extract the month from date_harvested
    days_difference = as.numeric(as.Date(date_harvested, format = "%m/%d/%y") - as.Date(date_planted, format = "%m/%d/%y"))
  ) %>%
  # Remove columns
  select(
    -replicate, -block, -year.y, -site.y, -year.x, -site.x,
    -grain_moisture, -year, -date_planted, -date_harvested, -year_site
  ) %>%
 #remove all NAs in data:
  filter(!is.na(month_harvested))

write_csv(df_model, "../data/training/df_model.csv")

#there are many NAs in this dataset. Soils data is incomplete, non-US sites (like the Germany site) do not have weather data, and some sites are missing lat/lon coordinates. This is OK for modelling, We will impute in the recipe.
```

## 3. Pre-processing

```{r Pre-processing}
# Seed:
set.seed(123)

# Split the data
df_split <- initial_split(df_model, prop = 0.7, strata = yield_mg_ha)
df_train <- training(df_split) %>%
  select(-c(month_planted, month_harvested, days_difference))
df_test <- testing(df_split)

# Recipe:
df_recipe <- recipe(yield_mg_ha ~ ., data = df_train) %>%
  step_novel(all_nominal_predictors()) %>%         # Handle new lvls 
  step_unknown(all_nominal_predictors()) %>%      # Handle unknown lvls
  step_other(all_nominal_predictors(), threshold = 0.001) %>% # Group rare levels
  step_dummy(all_nominal_predictors()) %>%        # hotcodin' predictors
  step_nzv(all_predictors()) %>%                  # nzv on predictors
  step_impute_median(all_numeric_predictors())     # IMPUTE

prep(df_recipe) %>% # Prep recipe
bake(new_data = NULL) %>% # Apply the recipe to train
  select(-ends_with("_other"))
```

## 4. XGBoost

```{r XGBoost}

# spec:
xgb_spec <- boost_tree(
  trees = tune(),         
  tree_depth = 5,         
  min_n = 10,              
  learn_rate = 0.1         
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Create the workflow
xgb_workflow <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(df_recipe)

# 5 fold
resampling_foldcv <- vfold_cv(df_train, v = 5)

# tuning grid:
xgb_grid <- grid_latin_hypercube(
  trees(range = c(50, 200)), 
  size = 4                   
)

# Tune the model
xgb_res <- tune_grid(
  object = xgb_workflow,
  resamples = resampling_foldcv,
  grid = xgb_grid,
  control = control_race(verbose = TRUE, save_pred = TRUE)
)
#save rds: (backup)
saveRDS(xgb_res, "../output/xgb_res.rds")
xgb_res <- readRDS("../output/xgb_res.rds")

#rmse
best_rmse_xgb <- xgb_res %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_xgb")
best_rmse_xgb

#r2
best_r2_xgb <- xgb_res %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_xgb")
best_r2_xgb
#pct loss
best_rmse_pct_loss_xgb <- xgb_res %>%
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1) %>%
  mutate(source = "best_rmse_pct_loss_xgb")
best_rmse_pct_loss_xgb
#rmse based on 1 std error:
best_rmse_one_std_err_xgb <- xgb_res %>%
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_rmse_one_std_err_xgb")
best_rmse_one_std_err_xgb
#based on this, the rmse one will be chosen, since 3/4 bests are the same model.

# Extract best hyperparams
best_params <- best_rmse_xgb

# Finalize the XGBoost specification with best hyperparams
xgb_final_spec <- boost_tree(
  trees = best_params$trees,
  tree_depth = 5,
  min_n = 10,
  learn_rate = 0.1
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Finalize the workflow with the best model
xgb_final_workflow <- workflow() %>%
  add_model(xgb_final_spec) %>%
  add_recipe(df_recipe)

# Fit wf
xgb_final_fit <- fit(xgb_final_workflow, data = df_train)

# Eval performance:
test_predictions <- predict(xgb_final_fit, new_data = df_test) %>%
  bind_cols(df_test) %>%
  mutate(
    rmse = sqrt(mean((yield_mg_ha - .pred)^2, na.rm = TRUE)),
    rsq = cor(yield_mg_ha, .pred, use = "complete.obs")^2
  )

# Print RMSE and R-squared
test_rmse <- test_predictions$rmse[1]
test_rsq <- test_predictions$rsq[1]
cat("Test RMSE:", test_rmse, "\n")
cat("Test R-squared:", test_rsq, "\n")

# # Calculate RMSE and R2
# rmse <- sqrt(mean((test_predictions$yield_mg_ha - test_predictions$.pred)^2, na.rm = TRUE))
# rsq <- cor(test_predictions$yield_mg_ha, test_predictions$.pred, use = "complete.obs")^2

# Plot actual vs. predicted yield
library(ggplot2)
results_xgb<- ggplot(test_predictions, aes(x = yield_mg_ha, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Actual vs Predicted Yield (mg/ha)",
    x = "Actual Yield",
    y = "XgBoost Predicted Yield"
  ) +
  # Add RMSE and R2
  annotate(
    "text",
    x = max(test_predictions$yield_mg_ha, na.rm = TRUE) * 0.2, 
    y = max(test_predictions$.pred, na.rm = TRUE) * 1.2,       
    label = paste0("RMSE: ", round(rmse, 2), "\nR²: ", round(rsq, 2)),
    color = "black",
    size = 4,
    hjust = 0
  ) +
  theme_minimal()
ggsave("../output/xgb_results.png", plot = results_xgb, width = 8, height = 6)
```

## 5. Random Forest

```{r Random Forest}

#finding mtry:
num_predictors <- ncol(df_train) - 1  # everything but yield
typical_mtry <- round(sqrt(num_predictors))
#spec:
rf_spec <- rand_forest(
  mtry = 8,  # hardcoded as 8 based on sqrt of ncol df_train
  trees = tune(),       # Only tune trees
  min_n = 5
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("regression")

# 5 fold:
cv_folds <- vfold_cv(df_train, v = 5)

# tuning grid:
rf_grid <- grid_regular(
  trees(range = c(50, 500)),
  levels = 5                 
)
tictoc::tic("rf_tuning")

# Tune the model
rf_tuned <- tune_grid(
  object = rf_spec,
  preprocessor = df_recipe,
  resamples = cv_folds,
  grid = rf_grid,
  control = control_race(verbose = TRUE, save_pred = TRUE)
)
tictoc::toc()
# Notify when tuning is complete
beepr::beep("wilhelm")
# Save the tuned model results
saveRDS(rf_tuned, "../output/rf_tuned.rds")
rf_tuned <- readRDS("../output/rf_tuned.rds")
# Visualize tuning results
autoplot(rf_tuned)

best_rmse_rf <- rf_tuned %>%
  select_best(metric = "rmse") %>%
  mutate(source = "best_rmse_rf")
best_rmse_rf
#r2
best_r2_rf <- rf_tuned %>%
  select_best(metric = "rsq") %>%
  mutate(source = "best_r2_rf")
best_r2_rf
#best r2 1 std
best_r2_one_std_rf <- rf_tuned %>%
  select_by_one_std_err(metric = "rsq",
                        eval_time = 100,
                        trees) %>%
  mutate(source = "best_r2_one_std_rf")
best_r2_one_std_rf
# Select the best model based on RMSE (since 2/3 chose same model):
best_model <- best_rmse_rf
# Finalize random forest model with best parameters
final_rf <- finalize_model(rf_spec, best_model)

# Create a workflow with final model and recipe
wf <- workflow() %>%
  add_model(final_rf) %>%
  add_recipe(df_recipe)

# Fit final workflow on full training data
final_fit <- fit(wf, data = df_train)

vip(final_fit$fit$fit)

# Evaluate performance on test set
test_predictions_rf <- predict(final_fit, new_data = df_test) %>%
  bind_cols(df_test) %>%
  mutate(
    rmse = sqrt(mean((yield_mg_ha - .pred)^2, na.rm = TRUE)),
    rsq = cor(yield_mg_ha, .pred, use = "complete.obs")^2
  )

# Print RMSE and R2
test_rmse <- test_predictions_rf$rmse[1]
test_rsq <- test_predictions_rf$rsq[1]
cat("Test RMSE:", test_rmse, "\n")
cat("Test R-squared:", test_rsq, "\n")

# # Calculate RMSE and R^2
# rmse <- sqrt(mean((test_predictions_rf$yield_mg_ha - test_predictions_rf$.pred)^2, na.rm = TRUE))
# rsq <- cor(test_predictions_rf$yield_mg_ha, test_predictions_rf$.pred, use = "complete.obs")^2

# Plot actual vs. predicted yield
library(ggplot2)
results_rf <- ggplot(test_predictions_rf, aes(x = yield_mg_ha, y = .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Actual vs Predicted Yield (mg/ha)",
    x = "Actual Yield",
    y = "Random Forest Predicted Yield"
  ) +
  # Add RMSE and R2:
  annotate(
    "text",
    x = max(test_predictions_rf$yield_mg_ha, na.rm = TRUE) * 0.2, 
    y = max(test_predictions_rf$.pred, na.rm = TRUE) * 1.2,     
    label = paste0("RMSE: ", round(rmse, 2), "\nR²: ", round(rsq, 2)),
    color = "black",
    size = 4,
    hjust = 0
  ) +
  theme_minimal()
ggsave("../output/rf_results.png", plot = results_rf, width = 8, height = 6)
```

## 6. Variable Importance

```{r Variable Importance}
prep<- prep(df_recipe)

# XGBoost :
final_xgb_plot<- xgb_final_spec %>%
  fit(yield_mg_ha ~ ., data = bake(prep, new_data = df_train)) %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

ggsave("../output/variable_importance_xgb.png", plot = final_xgb_plot, width = 8, height = 6, dpi = 300)

#rf:
final_rf_plot <- final_rf %>%
  fit(yield_mg_ha ~ ., data = bake(prep, new_data = df_train)) %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

#save as object:

ggsave("../output/variable_importance_rf.png", plot = final_rf_plot, width = 8, height = 6, dpi = 300)

```

## 7. Predicting on Test Data

```{r - Test Submission Predictions}
#do same workflow but with test data: 

clean_site_names <- function(site_col) { #to clean names
  site_col %>%
    # remove replicates like IAH1a
    gsub("([A-Z]+\\d+)\\w?$", "\\1", .) %>%
    # remove rep numbers and spaces
    gsub("-? ?rep \\d+$", "", .) %>%
    # Remove dry early and late
    gsub("- ?(Dry|Early|Late)$", "", .)
}

#meta:
test_meta <- read.csv("../data/testing/testing_meta.csv") %>%
  mutate(site = clean_site_names(site))
test_meta$year_site <- paste0(test_meta$site, "_", test_meta$year)
#does not have TXH4

#trait:
test_trait <- read.csv("../data/testing/testing_submission.csv") %>%
  mutate(site = clean_site_names(site))

#soil:

test_soil <- read.csv("../data/testing/testing_soil.csv") %>%
  mutate(
    year_site = site,
    year_site = gsub("_(\\d+)_", "_", year_site)
  ) %>%
  select(-c(year, site))

test_soil_avg <- test_soil %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop") %>%
  ungroup() 


test_trait$year_site <- paste0(test_trait$site, "_", test_trait$year)



test_meta_reduced <- test_meta %>%
  group_by(year_site) %>%
  summarise(
    latitude = mean(latitude), 
    longitude = mean(longitude),
    previous_crop = first(previous_crop),
    year = first(year),
    site = first(site),
    .groups = "drop"
  )

# # Pull weather data only once per site
daymet_test <- test_meta_reduced %>%
  mutate(weather = pmap(list(.y = year,
                             .site = site,
                             .lat = latitude,
                             .lon = longitude),
                        function(.y, .site, .lat, .lon)
                          try({
                            download_daymet(
                              site = .site,
                              lat = .lat,
                              lon = .lon,
                              start = .y,
                              end = .y,
                              simplify = TRUE,
                              silent = TRUE
                            ) })))
#join:
test_trait <- left_join(test_trait, test_meta_reduced, by = "year_site")
#factor prev crop: 
test_trait$previous_crop <- as.factor(test_trait$previous_crop)
#join:
df_test <- left_join(test_trait, test_soil_avg, by = "year_site")

na.omit(df[1:15])
#clean up previous crop:
df_test <- df_test %>%
  mutate(previous_crop = tolower(previous_crop)) %>%
  mutate(previous_crop = case_when(
           is.na(previous_crop) ~ NA_character_,
           str_detect(previous_crop, "soy") ~ "soybean",
           str_detect(previous_crop, "corn") ~ "corn",
           str_detect(previous_crop, "cotton") ~ "cotton",
           str_detect(previous_crop, "wheat") ~ "small grain",
           str_detect(previous_crop, "sorghum") ~ "sorghum",
           str_detect(previous_crop, "rye") ~ "small grain",
           str_detect(previous_crop, "peanut") ~ "peanut",
           str_detect(previous_crop, "beet") ~ "sugar beet",
           str_detect(previous_crop, "fallow") ~ "fallow",
           str_detect(previous_crop, "lima bean") ~ "small grain",
           str_detect(previous_crop, "pumpkin") ~ "soybean/pumpkin", #mixed
           str_detect(previous_crop, "small grain") ~ "small grain",
           TRUE ~ "other"  # if theres any other
         )) %>%
  filter(!str_detect(year_site, "^GEH|ONH")) 

#gathering data: dropping data that doesn't work within daymet boundaries.
daymet_all_test <- daymet_test %>%
  filter(!map_lgl(weather, inherits, "try-error")) %>%
  unnest(weather, names_sep = "_") %>%
  group_by(
    site,
    year,
    yday = weather_yday,
    weather_measurement,
    latitude = weather_latitude,
    longitude = weather_longitude,
    altitude = weather_altitude
  ) %>%
  summarise(
    value = mean(weather_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = weather_measurement,
    values_from = value
  ) %>%
  clean_names()

 daymet_all_test %>%
  # Selecting needed variables
  dplyr::select(year, site, latitude, longitude,
                yday,
                dayl.s = dayl_s,
                prcp.mm = prcp_mm_day,
                srad.wm2 = srad_w_m_2,
                tmax.c = tmax_deg_c,
                tmin.c = tmin_deg_c,
                vp.pa = vp_pa
                )

daymet_all_test %>%
  pivot_longer(cols = dayl_s:vp_pa) %>%
  ggplot(aes(x = value)) + 
  geom_density() +
  facet_wrap(.~name, scales = "free")


daymet_all_test <- daymet_all_test %>%
  mutate(date_chr = paste0(year, "/", yday)) %>%
  mutate(date = as.Date(date_chr,"%Y/%j")) %>%
  mutate(month = month(date)) %>%
  mutate(month_abb = format(date, "%b"))

training_weather_fe_test <- daymet_all_test %>%
  group_by(year, site, month, month_abb) %>%  # include month_abb here
  summarise(
    across(.cols = c(dayl_s, srad_w_m_2, tmax_deg_c, tmin_deg_c, vp_pa),
           .fns = mean,
           .names = "mean_{.col}"),
    across(.cols = prcp_mm_day,
           .fns = sum,
           .names = "sum_{.col}")
  ) %>%
  ungroup()

#pivot:
training_weather_wide_test <- training_weather_fe_test %>%
  filter(month >= 3 & month <= 10) %>% #filtering by planting to harvest months 
  pivot_longer(cols = mean_dayl_s:sum_prcp_mm_day) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  dplyr::select(-name, -month_abb) %>%
  pivot_wider(names_from = varname,
              values_from = value) %>%
  mutate(across(c(4:50), ~round(.,1)))


# Add site_year
training_weather_wide_test <- training_weather_wide_test %>%
  mutate(year_site = paste0(site, "_", year))


# summarize weather to 1 row per year_site
training_weather_wide_summary_test <- training_weather_wide_test %>%
  select(-month) %>%
  group_by(year_site) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")


# join weather and df
df_full_test <- df_test %>%
  left_join(training_weather_wide_summary_test, by = "year_site") %>%
 # select(-c("year.x", "previous_crop", "year.y")) %>%
  mutate(year_site = gsub("TXH1-(Dry|Early|Late)", "TXH1", year_site))

#BAKE test data:
baked_df_test <- prep(df_recipe) %>% # Prep
bake(new_data = df_full_test) %>% # Apply to the training data
  select(-ends_with("_other")) # Remove "_other" columns

#XGB:
#----------------------------------------------------------------------------------
# populate predictions
test_sub <- read_csv("../data/testing/testing_submission.csv")
predictions <- predict(xgb_final_workflow, df_full_test) %>%
  bind_cols(df_test) 
predictions <- predictions %>%
  mutate(yield_mg_ha = .pred) %>%
  mutate(year = year.x) %>%
  mutate(site = site.x)
test_sub_updated <- test_sub %>%
  left_join(predictions %>% select(year, site, hybrid, yield_mg_ha), 
            by = c("year", "site", "hybrid")) %>%# Populate 
  select(-yield_mg_ha.x) %>%
  mutate(yield_mg_ha = yield_mg_ha.y) %>%
  select(-yield_mg_ha.y)

# save:
write_csv(test_sub_updated, "../output/testing_submission_xgb_WILLIS.csv")

#RF:
#----------------------------------------------------------------------------------
# populate predictions

#same flow:
predictions_rf <- predict(final_fit, df_full_test) %>%
  bind_cols(df_test) 
predictions_rf <- predictions_rf %>%
  mutate(yield_mg_ha = .pred) %>%
  mutate(year = year.x) %>%
  mutate(site = site.x)
test_sub_updated_rf <- test_sub %>%
  left_join(predictions_rf %>% select(year, site, hybrid, yield_mg_ha), 
            by = c("year", "site", "hybrid")) %>%# Populate
  select(-yield_mg_ha.x) %>%
  mutate(yield_mg_ha = yield_mg_ha.y) %>%
  select(-yield_mg_ha.y)
#save
write_csv(test_sub_updated_rf, "../output/testing_submission_rf_WILLIS.csv")

```
